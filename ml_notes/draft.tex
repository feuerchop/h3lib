	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2016 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2016,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}
\usepackage{nips15submit_e,times}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%-------------------------------------------------------------
%                      Own Commands
%-------------------------------------------------------------
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\cbn}{\textsc{Cbn}}
\newcommand{\bn}{\textsc{Bn}}

\renewcommand{\algorithmiccomment}[1]{/* #1 */}
\def\ci{\perp\!\!\!\perp}
\def\dep{\perp\!\!\!\perp\!\!\!\!\!\!\!/\,\,\,\,}
% Theorem & Co environments and counters
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{equat}[theorem]{Equation}
\newtheorem{example}[theorem]{Example}
%-------------------------------------------------------------
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etal}{\emph{et al.}}
\newcommand{\etc}{\emph{etc.}}
\newcommand{\cf}{\emph{cf.}}
\newcommand{\diff}[2]{\frac{\partial #1}{\partial #2}}
%\newcommand{\diff}[2]{\nabla_{#2}{#1}}
\newcommand{\vct}[1]{\ensuremath{\boldsymbol{#1}}} %for greek letters
\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\set}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\con}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\bb}[1]{\ensuremath{\mathbb{#1}}}
\newcommand{\T}{\ensuremath{\top}}
\newcommand{\mycomment}[1]{\textcolor{red}{#1}}
\newcommand{\ind}[1]{\ensuremath{\mathbbm 1_{#1}}}
\newcommand{\argmax}{\operatornamewithlimits{\arg\,\max}}
\newcommand{\erf}{\text{erf}}
\newcommand{\argmin}{\operatornamewithlimits{\arg\,\min}}
\newcommand{\bmat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\question}[1]{\textcolor{red}{Q: #1}}


\title{Machine Learning Working Notes}

\author{
	Huang Xiao\thanks{Visiting homepage: feuerchop.github.io} \\
	Department of Computer Science\\
	Technical University of Munich\\
	Munich, Germany 85748 \\
%	\texttt{hippo@cs.cranberry-lemon.edu} \\
%	\And
%	Coauthor \\
%	Affiliation \\
%	Address \\
%	\texttt{email} \\
%	\AND
%	Coauthor \\
%	Affiliation \\
%	Address \\
%	\texttt{email} \\
%	\And
%	Coauthor \\
%	Affiliation \\
%	Address \\
%	\texttt{email} \\
%	\And
%	Coauthor \\
%	Affiliation \\
%	Address \\
%	\texttt{email} \\
%	(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle


\begin{abstract}
	Machine learning is a fast pacing discipline in many working fields, especially it is now regarded as the most impacting subject in artificial intelligence. In this working notes, I summarize some important notes during my study of machine learning. For the completeness, references are included for readers who are reading this article. Note that this working note is only distributed and shared with author's acknowledge and confirmation. It is not intended as a publishable research paper or tutorial.

\end{abstract}
\section{Linear Models}
\subsection{Regression}
It seems everything starts to grow from linear model, whatever regression or classification, linear models expand to almost many other learning models we face during the research. So starting from a very simple linear regression problem, given training set $\set D = \left\{ \vct x_i, \vct y_i \right\}_{i=1}^n $ with $ n $ sample, where $ \vct y $ are the responses as numerical values. A linear regression model finds a linear weight vector, which minimizes a certain type of empirical error. This is obviously defined in perspective of statistical learning theory. 
\begin{equation}
\argmin_{\vct w} \frac{1}{n}\sum_{i=1}^n \| w^T x_i + b - y_i\|^2
\end{equation}
where empirical error introduced by individual sample is equally weighted by $ 1/n $. It is seen that we are using a straight line to fit a possibly any shaped function, obviously the summation or mean error can be large due to the noise or intrinsic nonlinearity of function. However, a typical misunderstanding of linear model for beginners is that the term linear refers to $ \vct w $ instead of $ \vct x $. That is to say, we are expecting a linear model on parameters $ \vct w $, but the feature vectors $ \vct x $ can actually be any shape. Therefore, in literature we mostly see a feature mapping of input $ \vct x $ as $ \phi(\vct x) $, and it does not break the linear property of the model. Therefore, we have our liner model as,
\begin{equation}
\argmin_{\vct w} \frac{1}{n}\sum_{i=1}^n \| w^T \phi(x_i) + b - y_i\|^2
\label{eq:linear_reg}
\end{equation}
A typical feature mapping is polynomial feature mapping. Suppose we have a 2-dimensional input data sample $ \left( x_1, x_2 \right) $, we define a feature mapping as follows,
\[ \left( x_1, x_2 \right) \rightarrow \left( x_1, x_2, x_1^2+x_2^2 \right)\], where a 2-d plane is transformed as a paraboloid in 3-d space. Substituting back into previous linear function, it becomes a polynomial line fitting problem, but it is still linear in $ \vct w $.

To solve the least square problem defined in Eq. \ref{eq:linear_reg} to obtain an optimal parameter estimation $ \vct w $, we take the gradient with respect to the loss and set it to zero. Note that the intercept $ b $ can be folded in vector $ \vct w $ by adding additional entry $1$ in the end, for simplicity, we ignore it in our formulation. The least square solution is,
\begin{equation}
\vct w^* = \left( \Phi(\vct X)^T\Phi(\vct X) \right)^{-1}\Phi(\vct X)^T\vct Y
\label{eq:lsq_sol}
\end{equation}
As long as the $ \Phi(\vct X)^T\Phi(\vct X) $ is not singular, there exists analytical solution. We will call $ \Phi(\vct X) $ design matrix, which takes each row as a feature mapping on $ \vct x $. We will see in short that the inner product of feature mapping can be explicitly defined by a certain kernel function, which established a very important chapter of learning theory, \ie \textit{Kernel Methods}. To predict the response for a new sample $ \vct x^* $, we have,
\[ \vct y^* = \phi(\vct x^*) \left( \Phi(\vct X)^T\Phi(\vct X) \right)^{-1}\Phi(\vct X)^T\vct Y \]

\subsubsection*{From Probabilistic View}
Different from minimizing empirical errors from observations, we can examine the whole problem in a probabilistic view, that is, minimize the uncertainty from observations. If we reformulate the problem as a summation of a deterministic function and a indeterministic noise from a certain probabilistic distribution, we can write the linear model as,
\[ \vct y = \vct w^T \phi(\vct x) + \epsilon \], where $ \epsilon \sim \set N\left( 0, \sigma^2 \right)$ which is defined as a Gaussian noise, the bias term $ b $ is again folded in $ \vct w $. Moreover, we can get the response $ \vct Y $ as a Gaussian distribution as well. 
\[ \vct Y \sim \prod_{i=1}^n \set N\left( \vct w^T\phi(\vct x_i), \sigma^2 \right) \]
In order to get the optimal parameter $ \vct w $, we need to maximize the likelihood $ p\left( \vct Y\mid \vct X, \vct w \right) $. It equals to maximize the log-likelihood, and the log-likelihood gives,
\begin{equation}
\ln p\left( \vct Y \mid \vct X, \vct w \right) = -\frac{n}{2}\ln(2\pi)-n\ln(\sigma) - \frac{1}{2\sigma^2}\|\vct Y - \Phi(\vct X)\vct w\|^2
\label{eq:mle}
\end{equation}
From Eq.\ref{eq:mle}, we can see maximizing the log-likelihood (\textit{MLE}) equals minimizing the least squared error. We will get the exact same solution as in Eq.\ref{eq:lsq_sol}. 

\subsubsection*{Overfitting}
Now suppose we have four 1-dimensional observations $ \vct X $, and define an arbitrary feature mapping $ \phi $, we expect to find a linear model to minimize the least squared error, as we see in \eqref{eq:linear_reg}. 
\[ \bmat{2 \\ 1 \\-1} = \phi\left( \bmat{1.5 \\ 0.5 \\ 2.5} \right)  \vct w \]
If we define the feature mapping $ \phi $ as 
\[  \bmat{1.5 \\ 0.5 \\ 2.5} \xrightarrow{\phi} \bmat{1 & 0 & 0 \\ 0 & 1 & 0\\ 0 & 0 & 1}\]
We can see that $ \vct w $ is exactly the response vector $ \vct y $, and the least squared error is minimized as zero. A feature mapping can always be defined to achieve zero error, if there's no constraints at all. But obviously, there is no benefit to use this linear model on any prediction task, and mostly likely we will get a very high prediction error based on that. If the model performs well on training dataset, but poorly on unseen data, we can call this situation as overfitting. And certainly, overfitting is a central problem that machine learning attempts to solve. 

To avoid overfitting, we can firstly introduce constraint by adding a penalty term on the complexity of the $ \vct w $, which is known as \textit{regularization}. For example, by penalizing a 2-norm of $ \vct w $, we can generalize the least squared error problem as,
\begin{equation}
\argmin_{\vct w} \frac{1}{n}\sum_{i=1}^n \| \vct w^T \phi(x_i) + b - y_i\|^2 + \frac{\lambda}{2}\vct w^T\vct w 
\label{l2norm}
\end{equation}
Similarly taking the derivative w.r.t. $ \vct w $ and set to zero, we can get the optimal parameters as,
\begin{equation}
\vct w^* = \left( \Phi(\vct X)^T\Phi(\vct X) + \lambda \vct I \right)^{-1}\Phi(\vct X)^T\vct Y
\label{eq:lsq_reg_sol}
\end{equation}
Again from probabilistic view, we introduce uncertainty on $ \vct w $ instead of only considering uncertainty on response $ \vct y $. Define a prior on $ \vct w $ following a D-dimensional Gaussian distribution,
\[ \vct w \sim \set N\left( \vct 0, \Sigma_{\vct w} \right) \] 
We want to capture the posterior distribution on $ \vct w $ after observations of $ \left( \vct X, \vct Y \right) $, that is, the objective is to maximize the posterior according to Bayes theorem,
\begin{equation}
\max p\left( \vct w \mid \vct X, \vct Y \right) = \frac{p\left( \vct Y \mid \vct X, \vct w \right)p\left( \vct w \right)}{\int p\left( \vct Y \mid \vct X, \vct w \right)p\left( \vct w \right)d\vct w}
\label{eq:posterior}
\end{equation}
The denominator in Eq.\ref{eq:posterior} is also called marginal likelihood, which is independent of $ \vct w $, therefore, taking the logarithm of the posterior, we have,
\begin{equation}
\ln p\left( \vct w \mid \vct X, \vct Y \right) = -\frac{(n+d)}{2}\ln(2\pi)-n\ln(\Sigma_{\vct w}) - \frac{1}{2}\ln|\Sigma_{\vct w}|- \frac{1}{\sigma^2}\|\vct Y - \Phi(\vct X)\vct w\|^2 - \frac{1}{2}\vct w^T\Sigma_{\vct w}^{-1}\vct w 
\label{eq:ln_posterior}
\end{equation}
Take the gradient w.r.t. $ \vct w $ and set to zero, we can get very similar results as in Eq.\ref{eq:lsq_reg_sol}.
\begin{equation}
\vct w_{map} = \left( \Phi(\vct X)^T\Phi(\vct X) + \sigma^2\vct \Sigma_{\vct w}^{-1} \right)^{-1}\Phi(\vct X)^T\vct Y
\label{map_sol}
\end{equation}
If the prior is defined with an isotropic Gaussian, we see that the \textit{MAP} solution is equivalent to $ \ell_2 $ regularization form. Now let us look back at the posterior, note that,
\begin{align}
p\left( \vct w \mid \vct X, \vct Y \right) & \propto p\left( \vct Y \mid \vct X, \vct w \right)p\left( \vct w \right)  \nonumber\\
& \propto \exp \left\lbrack -\frac{1}{2\sigma^2}\left( \Phi(\vct X)\vct w - \vct Y\right)^T\left( \Phi(\vct X)\vct w - \vct Y\right) \right\rbrack \exp \left( \frac{1}{2}\vct w^T\Sigma_{\vct w}^{-1}\vct w \right)  \nonumber\\
& \propto \exp\left\{ -\frac{1}{2}\vct w^T\left( \frac{1}{\sigma^2}\Phi(\vct X)^T \Phi(\vct X) + \Sigma_{\vct w}^{-1}\right)\vct w + \frac{1}{\sigma^2}\vct Y^T\Phi(\vct X)\vct w \right\}
\end{align}
By completing the square we can get the mean and covariance of the posterior, that is, 
\begin{align}
 \vct w^* &\sim\set N \left( \vct m^*,  \vct A^{-1} \right) \nonumber \\
 \vct m^* &= \frac{1}{\sigma^2} \vct A^{-1} \Phi(\vct X)^T\vct Y \\
  \vct A  &= \frac{1}{\sigma^2}\Phi(\vct X)^T\Phi(\vct X)+\Sigma_{\vct w}^{-1}
\end{align}
And we see that the \textit{MAP} solution is exactly the same as the mode of the posterior. To predict a new input sample $ \vct x^* $, we can derive a predictive distribution instead of just a single value at the mode, and it is again a Gaussian with posterior mean multiplied with the test sample, and variance of the predictive distribution is the quadratic form on the posterior covariance, which grows with the magnitude of test samples. 
\begin{align} 
	p(\vct y^* \mid \vct X, \vct Y, \vct x^*) & = \int p(\vct y^*\mid \vct X, \vct Y, \vct w, \vct x^*)p(\vct w \mid \vct X, \vct Y)d\vct w  \nonumber \\
	& \sim \set N\left( \vct \Phi(\vct x^*)^T \vct m^*, \Phi(\vct x^*)^T \vct A^{-1} \Phi(\vct x^*) \right)
\end{align}


\subsection{Classification}
Now let us consider classification problem, where we expect a functional $ \pi(\vct x) $ to map input $ \vct x $ to class labels, in binary case $ \vct y=\left( +1, -1 \right) $. More commonly, we can define a Bernoulli distribution $ p(y=+1 \mid x) $ for one class and $ 1-p\left( y=+1 \mid x \right) $ for another. Typically, we would choose a sigmoid function, \eg logistic function or \textit{tanh} to warp a possibly infinite value into a bounding box, \eg, $ \left\lbrack 0, 1 \right\rbrack $ for logistic function. This is a desired behavior, since any function value will be transformed to a probability. A logistic function is defined,
\[ \sigma(a) = \frac{1}{1+\exp(-a)} \]
Obviously, we also have, 
\begin{align*}
\sigma(-a) = 1-\sigma(a) \\
\tanh(a) = 2\sigma(2a) -1 \\
\diff{\sigma(a)}{a} = \left( 1-\sigma(a) \right)\sigma(a)
\end{align*}
Thus, the conditional class distribution given input dataset can be defined as a sigmoid function on the linear model $ p\left( y \mid \vct x \right) = \sigma(yf(\vct x))$, again we fold the bias term $ b $ in the parameters $ \vct w $. 

\section{Gaussian Process}
\subsection{Regression}
\label{sec:gpr}
Gaussian process is an important nonparametric regression model which looks for an optimal functional in a space of functions, that minimizes a loss function, although the loss function needs not to be explicitly defined. \cite{rw06gp}

Give a training dataset $\set D = \left\{x_i\right\}_{i=1}^n$ with $x_i \in R^d$, \textit{i.i.d} drawn from certain distribution, we are interested at the predictive distribution of unknown target for the  test sample $x_*$, denoted as $f_*$. Suppose a prior over $\vct y$ given input $\vct X$ is a $n$-variable Gaussian distribution,
\[
\vct y \sim \set N(\vct 0, K(\vct X, \vct X))
\]
where $K(\vct X, \vct X)$ defines a covariance function over $\vct X$. Therefore the posterior of $f_*$ given the training dataset $\set D$ is also a Gaussian.
\[
f_*|\vct y, \vct X, x_* \sim \set N(\mu_*, \Sigma_*^{-1})
\]
where the sufficient statistics can be derived using Bayesian theorem,
\begin{align}
	& \bar{f_*} = \vct k_{*}^T (\vct K +\sigma_n^2 I)^{-1}\vct y \label{eq:gpr_mu}\\
	& \bb{V}(f_*) = k(\vct x_*, \vct x_*) - \vct k_{*}^T (\vct K+\sigma_n^2 I)^{-1} \vct k_{*}  \label{eq:gpr_cov}
\end{align}
where $\sigma_n^2$ is the noise level, $\vct K$ represents a shorthand for covariance matrix on input $ \vct X $, and we denote $ \vct k_{*} $ as the kernel function $ k(\vct X, \vct x_*) $ for simplicity.
%\question{Here comes the question of how to estimate the parameters for the covariance $K$.}

To obtain the Eq.\eqref{eq:gpr_mu}-\eqref{eq:gpr_cov}, we can use the following trick.
Given two variables $(\vct x, \vct y)$ following a Gaussian distribution,
\[
\bmat{\vct x\\ \vct y} \sim \set N\left(
\bmat{\mu_x \\ \mu_y}, \bmat{A & C \\ C^T & B}
\right)
\]
Then we have the conditional distribution,
\[
\vct x|\vct y \sim \set N\left(\mu_x+CB^{-1}(\vct y - \mu_y), A - C^TB^{-1}C \right)
\]

Now looking at the predictive mean of training dataset, which can be given by Eq.\eqref{eq:gpr_mu}-\eqref{eq:gpr_cov} on training set $ \vct X $ itself, 
\[ \bar{f} = \vct K(\vct K + \sigma_n^2 I)^{-1}\vct y \]

Since $ \vct K $ is symmetric positive definite and its eigendecomposition is $ \vct K=\sum_{i=1}^n \lambda_i\vct u_i \vct u_i^T$, where $ \lambda_i $ is the \textit{i}th eigenvalue and $ \vct u_i $ is the \textit{i}th eigenvector. Now define a vector $ \vct U=\bmat{\vct u_1, \ldots, \vct u_n} $, therefore we have $ \vct K = \vct U\Sigma\vct U^T $. According the the matrix inverse lemma [\cite{mil86}], we can derive a simple form for the predictive mean, where we observe that the predictive mean is a linear smooth on their targets. 

\begin{align*}
\bar{f} & = \vct K\left( \sigma_n^{-2}I - \sigma_n^{-2}I\vct U \left( \Sigma^{-1}+\vct U^T\sigma_n^{-2}I\vct U \right)^{-1}\vct U^T\sigma_n^{-2}I\right)\vct y \\
& = \vct K\left( \sigma_n^{-2}I - \sigma_n^{-2}\vct U \left( \Sigma^{-1}+\sigma_n^{-2}I \right)^{-1}\vct U^T\sigma_n^{-2}\right)\vct y \\
& = \left( \sigma_n^{-2}\vct U\Sigma\vct U^T - \vct U\Sigma
			\bmat{\frac{\lambda_1\sigma_n^{-2}}{\lambda_1+\sigma_n^{2}} & & \\ 
						& \ddots & \\ 
						& & \frac{\lambda_n\sigma_n^{-2}}{\lambda_n+\sigma_n^{2}}}
		\vct U^T \right)\vct y \\
& = \left( \sigma_n^{-2}\vct U\Sigma\vct U^T - \sigma_n^{-2}\vct U
\bmat{\frac{\lambda_1^2}{\lambda_1+\sigma_n^{2}} & & \\ 
	& \ddots & \\ 
	& & \frac{\lambda_n^2}{\lambda_n+\sigma_n^{2}}}
\vct U^T \right)\vct y \\
& = \left( \sigma_n^{-2}\sum_{i=1}^n\lambda_i\vct u_i\vct u_i^T - \sigma_n^{-2}\sum_{i=1}^n \frac{\lambda_i^{2}}{\lambda_i + \sigma_n^2}\vct u_i\vct u_i^T \right)\vct y \\
& = \sum_{i=1}^n \frac{\lambda_i}{\lambda_i + \sigma_n^2}\vct u_i\vct u_i^T\vct y \\
& = \sum_{i=1}^n \frac{\gamma_i\lambda_i}{\lambda_i + \sigma_n^2}\vct u_i, \quad \text{with } \gamma_i = \vct u_i^T\vct y
\end{align*}

\subsection{Classification}
Using Gaussian process for classification task is a bit more complicated than regression. The main idea of it is to use a `squash' function to convert a predicted function value within $\left[ 0, 1\right]$, \eg, sigmoid function, cumulative Gaussian \textit{pdf}. Given a dataset $\set D = \left\{x_i\right\}_{i=1}^n$ with $x_i \in R^d$ and corresponding labels $\vct y=\left[ +1,-1 \right] $

\section{Support Vector Machines}


\section{PCA}
The basic idea of PCA is to maximally reduce information loss of projecting high dimensional data to lower dimension. Therefore, an intuitive consideration would be that we introduce first of all a projection matrix $\vct u$ on $d$-dimensional instance $x$, so that $x$ is mapped on a lower $m$-dimensional space. Following column vector routine, we expect that matrix $\vct u$ as being $m \times d$. Now given a dataset $ \vct X = \left\{  x_i\right\}_{i=1}^{n}$, it will be projected on a $ m $-dimensional space by $\vct u$. The objective of the projection is to maximize the covariance of data on the lower dimensional space, that is,
\[
	\max \dfrac{1}{n}\sum_{i=1}^{n}\|\vct ux - \vct u\bar{x}\|^2
\]
that can be rewritten as,
\begin{eqnarray}
	& \underset{\vct u}{\textit{maximize}}\quad \vct u S \vct u^T \nonumber \\
	\text{s.t.} & \vct u_i\vct u_i^T = 1, \,\, i=1,\ldots,m
	\label{eq:pca_obj}
\end{eqnarray}
where $S$ is the covariance of $\vct X$. To prevent $\vct u$ goes to infinity, we assume $\vct u$ has unit length, namely, $\vct u$ represents a set of basis of the lower dimension.

According to \eqref{eq:pca_obj}, we introduce $m$ Lagrangian multipliers $\vct \lambda$ as a diagonal matrix, and we have
\[ L = \underset{\vct u}{\textit{maximize}}\quad \vct u S \vct u^T - \vct\lambda \vct u\vct u^T \]

Take the derivative of $L$ with respect to $\vct u$, we have
\begin{eqnarray}
	\vct uS & = & \vct\lambda\vct u \nonumber \\
	S & = & \vct u^{-1}\lambda\vct u
	\label{eq:pca_sol}
\end{eqnarray}
Since $\vct u$ is orthogonal, it is therefore not singular. We see that $\vct u$ and $\vct lambda$ are the indeed the eigenvectors and eigenvalues for $S$ respectively. And Eq.\eqref{eq:pca_sol} is exactly the singular value decomposition of $S=U\Sigma V^T$, we can derive $\vct u$ and $ \vct{\lambda} $ from $S$ conveniently.

\bibliography{myrefs}
\bibliographystyle{icml2016}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz,
% slightly modified from the 2009 version by Kiri Wagstaff and
% Sam Roweis's 2008 version, which is slightly modified from
% Prasad Tadepalli's 2007 version which is a lightly
% changed version of the previous year's version by Andrew Moore,
% which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
